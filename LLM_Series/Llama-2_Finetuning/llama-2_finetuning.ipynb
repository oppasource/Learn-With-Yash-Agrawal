{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5201c00-2f90-42f5-8b1d-0763662c6704",
   "metadata": {},
   "source": [
    "# Fine-tuning llama-2 for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a35c9cb-7c8e-4f29-a1b4-3efc531f22b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/conda_env/yash/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767b93d-0fc2-4e0c-8454-9c9ae5680668",
   "metadata": {},
   "source": [
    "# Loading the Base Model to Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826bef1a-3b08-4712-9007-bc6d37591c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    cache_dir=\"/data/base_models/\",\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", cache_dir=\"/data/base_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52e6e9-cae3-419e-a71e-6fa40c69bb1a",
   "metadata": {},
   "source": [
    "# Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0f324f-bd3b-4177-9e32-13e63566d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"jonathansuru/customer_service_information_extraction\", cache_dir = \"/data/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1b975e-d2b1-4a2d-929f-d5c94522b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(example):\n",
    "    completion = re.sub('  +', '', example['completion'].strip())\n",
    "    completion = re.sub(',\\\"', ',\\n\\\"', completion)\n",
    "    completion = re.sub(',\\'', ',\\n\\'', completion)\n",
    "    example['complete_prompt'] = example['prompt'].strip() + '\\n\\n' + completion\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753be344-eb48-46f6-b4be-2e4308569d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ca1caa1-4da9-46b6-95e8-8f9fe04613f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion', 'complete_prompt'],\n",
       "        num_rows: 190\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3654ec8c-f75d-4ce3-966e-20bdc91349bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please extract the customer specifications from the conversation below:\n",
      "\n",
      "###\n",
      "\n",
      "Agent: Hello, thank you for calling [Company Name]. How may I help you today?\n",
      "Customer: I'm calling to complain about a product that I purchased.\n",
      "Agent: I understand. Can you please tell me what product you're referring to?\n",
      "Customer: I purchased a [Product Name] from your website on August 10th. It arrived on August 15th, but it was damaged.\n",
      "Agent: I'm sorry to hear that. I'll be happy to help you with this. Can you send me a picture of the damaged product?\n",
      "Customer: Sure.\n",
      "...\n",
      "Agent: I've received the picture of the damaged product. I'm going to issue you a refund for the product. I'm also going to send you a replacement product.\n",
      "Customer: Thank you for your help.\n",
      "The extract is as follows:\n",
      "\n",
      "\"product name\": \"product name\",\n",
      "\"issue\": \"damaged product\"\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['complete_prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e4ba9d-dc95-4c6a-a181-a0c0766d22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(example):\n",
    "    response = tokenizer(example['complete_prompt'])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b0eecb0-919c-4aa4-8627-5da66552a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30cf7791-b405-4a2e-a731-fe9ae6fd6532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion', 'complete_prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 190\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c0cd124-1517-43ca-ab8f-3247af261a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['train'].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393cf964-705c-4e7d-b50c-63c6cbe4b002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'complete_prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 171\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcdb6224-9a83-497f-b65f-f1ad0055d04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'complete_prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 19\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30421f6a-1b1f-4944-86e1-4f1121477193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "015b5155-aa04-4e6d-a4df-ee6f7de0c180",
   "metadata": {},
   "source": [
    "# Configuring model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6276d77e-6812-4ba4-a294-a8d05264716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c832a4-e588-498f-abd9-dd5fa5c5c7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b36d5da-c6ca-4418-bcad-b8b14c019b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "864a15cf-8602-4884-9dcd-2f3bd004f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 6738415616 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "277fa9b1-7c02-4aba-b94f-cabf3ce9b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "152d0d33-4707-4bdd-8b9d-140bc7158e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8388608 || all params: 6746804224 || trainable%: 0.12433454005023165\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b437e-cc7a-4b03-9e64-4361228f817c",
   "metadata": {},
   "source": [
    "We have added a small layer of trainable parameter to the base model. This layer will only be trained and the remaining whole model will remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1c1c3-23ce-466e-8950-7cecb76dec02",
   "metadata": {},
   "source": [
    "# Starting the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce4d5a06-ea73-4782-81e0-378fe6d704c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 04:06, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.640300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.631300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.505700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.431900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.813673057158788, metrics={'train_runtime': 256.8793, 'train_samples_per_second': 1.869, 'train_steps_per_second': 0.117, 'total_flos': 6063517055262720.0, 'train_loss': 0.813673057158788, 'epoch': 2.79})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=data['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4, \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=15, \n",
    "        max_steps=30, \n",
    "        learning_rate=1e-3, \n",
    "        # fp16=True,\n",
    "        logging_steps=1, \n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e3adf7d-2c65-4758-afc8-9474926debc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained('outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3945949-2402-4d8f-bdfd-a6f1dbd0d160",
   "metadata": {},
   "source": [
    "# Loading the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11c9b13-86d4-48b4-891e-8532d723b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:04<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "output_dir = './outputs'\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    cache_dir=\"/data/base_models/\",\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", cache_dir=\"/data/base_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da4cec-edca-428d-abf8-3b01ab06a37d",
   "metadata": {},
   "source": [
    "# Testing model output of fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4b1494-7847-4e5c-9dde-b5e099586971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama2_reponse(prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature= 0.00001)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7d86f98-98ce-42b6-9e79-f5c21b1c4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please extract the customer specifications from the conversation below: \n",
      "\n",
      "###\n",
      "\n",
      "Customer: I'm having trouble downloading an app from the Google Play Store.\n",
      "Agent: I'm sorry to hear that. Can you tell me what the problem is?\n",
      "Customer: The app is stuck at 0% download.\n",
      "Agent: Okay, I can help you with that. Can you please try restarting your device?\n",
      "Customer: I've tried restarting my device, but the app is still stuck at 0% download.\n",
      "Agent: Okay, I can try clearing the cache and data for the Google Play Store app.\n",
      "Customer: Okay, please do.\n",
      "Agent: Okay, I've cleared the cache and data for the Google Play Store app. Please try downloading the app again.\n",
      "Customer: The app is downloading now! Thank you so much for your help!\n",
      "The extract is as follows:\n",
      "\n",
      "\"problem\": \"app stuck at 0% download\",\n",
      "\"solution\": \"cleared cache and data for google play store app\"\n",
      "END OF EXTRACT\n",
      "END OF CONVERSATION\n",
      "END OF TRANSACTON\n",
      "END OF PRODUCT/SERVICE: APP DOWNLOAD\n",
      "END OF PLATFORM: GOOGLE PLAY STORE\n",
      "END OF PRODUCT/SERVICE: APP NAME\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "prompt = data['test']['prompt'][n]\n",
    "print(get_llama2_reponse(prompt, max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a224c79-122f-4fc7-970b-613b96fd729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \"product\": \"google play store\",    \"problem\": \"app stuck at 0% download\",    \"solution\": \"cleared cache and data for google play store app\"\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "print(data['test']['completion'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9f963-7373-4e67-b2ed-75d789f2860e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ae5562-f0cb-41e8-8246-619a2c37bde4",
   "metadata": {},
   "source": [
    "In the above example it had missed extracting the attribute \"product\" and also it looks like we need to add a stopping criteria. In our case whenever we encounter the word \"END\" it looks like that it our stopping criteria since our trianing dataset also contains \"END\" at the end of each trianing sample.\n",
    "\n",
    "This is still a good improvement given that we only got 171 trianing samples in the dataset and we only trained it for 30 epochs. Training parameters can further be tweaked to improve model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e4eec-2b0d-4955-a486-9e240eded2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
