{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e345083-05da-4ff9-894b-1f5c0e75fea5",
   "metadata": {},
   "source": [
    "# Comparison between Llama-2 and Llama-2-Chat\n",
    "\n",
    "Notebook is an experiment on differences between base Llama-2 and fine-tuned Llama-2-Chat mode. Both the models are loaded and prompted with the exact same prompts. The difference in the response is then observed. Llama-2-Chat is capable of generating EOS tags at right time so that it can stop generating at a logical point where base Llama-2 model starts generating some garbage even after answering the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eca9089-7852-4291-830a-714ba8fe9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e8ee81-55d7-4142-ae58-6a4ae17727ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7664df65e261496fb308bceb41688b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bcb09a752841b4a0e7d1f71a69ee6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    # cache_dir=\"/data/yash/base_models\",\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", \n",
    "                                          # cache_dir=\"/data/yash/base_models\"\n",
    "                                         )\n",
    "\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # cache_dir=\"/data/yash/base_models\",\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", \n",
    "                                               # cache_dir=\"/data/yash/base_models\"\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ba8f0e-dac5-4618-b426-3ed0cfcf6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama2_reponse(prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature= 0.00001)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def get_llama2_chat_reponse(prompt, max_new_tokens=50):\n",
    "    inputs = chat_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = chat_model.generate(**inputs, max_new_tokens=max_new_tokens, temperature= 0.00001)\n",
    "    response = chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1de7c1-e184-49e1-a5da-7b5878ca65cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She is a 2016 graduate of the University of North Carolina at Chapel Hill, where she majored in English and minored in Creative Writing. She is currently a graduate student at the University of North Carolina at Chapel Hill,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"She is\"\n",
    "get_llama2_reponse(prompt, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5105ca07-d87e-4337-bdad-05c8814a9e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q:what is the capital of India? A:New Delhi. Q:what is the capital of India? A:New Delhi. Q:what is the capital of India? A:New Delhi. Q:what is the capital of India? A:New Delhi. Q'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Q:what is the capital of India? A:\"\n",
    "get_llama2_reponse(prompt, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58e0a77-4a8a-479d-81ec-8070f6550d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q:what is the capital of India? A:The capital of India is New Delhi. Q:What is the currency of India? A:The currency of India is Indian rupee (INR). Q:What is the population of India? A:The population of India is approximately'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Q:what is the capital of India? A:\"\n",
    "get_llama2_chat_reponse(prompt, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6dde4d-b187-40df-b1db-784613741bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keep answer short. Q: what is the capital of India? A: New Delhi.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Keep answer short. Q: what is the capital of India?\"\n",
    "get_llama2_chat_reponse(prompt, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b33f91d-bf48-4942-a1df-a71e25277ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keep answer short. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi. Q: what is the capital of India? A: New Delhi'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying out same above prompt with base model to check the response\n",
    "\n",
    "prompt = \"Keep answer short. Q: what is the capital of India?\"\n",
    "get_llama2_reponse(prompt, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16b9a504-1af8-4fce-8b2d-ac3857fe9ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] Keep answer short. Q: what is the capital of India? [/INST]  The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"[INST] Keep answer short. Q: what is the capital of India? [/INST]\"\n",
    "get_llama2_chat_reponse(prompt, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7570cfdd-90bc-4a9f-a0a5-c5a1d9e1c67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] write a python code to print numbers from 1 to 10 without any explanation? [/INST]  Sure! Here is a simple Python code to print numbers from 1 to 10:\n",
      "```\n",
      "for i in range(1, 11):\n",
      "    print(i)\n",
      "```\n",
      "This will output:\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "\n",
      "I hope this helps! Let me know if you have any questions.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] write a python code to print numbers from 1 to 10 without any explanation? [/INST]\"\n",
    "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c02580-e52b-40c7-9514-8bdb9949fb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f838d4-d3d8-45db-8ab9-b5c2ecbb7a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f048eb6-94bc-4e56-880e-9ccab3ea80a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
